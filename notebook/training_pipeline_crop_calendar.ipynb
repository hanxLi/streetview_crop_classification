{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/cropClassification')\n",
    "sys.path.append('/workspace/cropClassification/model')\n",
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unet_uncertain import UNetWithUncertainty, UNetWithFiLM, UNetWithAttention, UNetWithAttentionDeep\n",
    "from unet import originalUNet\n",
    "from compiler import ModelCompiler\n",
    "from dataloader import RoadsideCropImageDataset\n",
    "from loss import AleatoricLoss, BalancedCrossEntropyLoss, BalancedCrossEntropyUncertaintyLoss #Picking between different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU with CUDA\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple M1/M2 GPU with MPS (Metal Performance Shaders)\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"type\": \"UNetWithUncertainty\", \n",
    "        \"params\": {\n",
    "            \"in_channels\": 10,  # Since we are using 9-channel input images\n",
    "            \"out_channels\": 3   # Number of output classes for segmentation\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 32,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"params\": {\n",
    "                \"lr\": 0.1\n",
    "            }\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"StepLR\",\n",
    "            \"params\": {\n",
    "                \"step_size\": 10,\n",
    "                \"gamma\": 0.8\n",
    "            }\n",
    "        },\n",
    "        \"criterion\": BalancedCrossEntropyLoss,\n",
    "        \"classwise_weights\": [0.14335682, 0.35709336, 0.49954982],\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 16\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_csv\": \"/workspace/data/masked_data_csiss/training/train_chipping_csv_w_anc.csv\",  # Path to the training DataFrame (includes npy file paths)\n",
    "        \"val_csv\": \"/workspace/data/masked_data_csiss/validation/validation_chipping_csv_w_anc.csv\", # Path to the validation DataFrame (includes npy file paths)\n",
    "        \"train_root_path\": \"/workspace/data/masked_data_csiss/training\",\n",
    "        \"val_root_path\": \"/workspace/data/masked_data_csiss/validation\",\n",
    "        \"image_column\": \"img_chip_path\",             # Column containing the image paths (npy files)\n",
    "        \"mask_column\": \"lbl_chip_path\",              # Column containing the mask paths\n",
    "        \"train_mean\": [121.65670372, 161.46653486, 158.23006705, 161.48324796, 227.81869621, \n",
    "                       146.17250505,  74.17987694,  98.37069385, 175.44668774, 0.59970105],  # Mean values for training set normalization\n",
    "        \"train_std\": [52.67447251,  55.39850822,  78.60574882,  53.93343019, 59.63657872, \n",
    "                      109.67295329,  29.33554407,  55.42471356, 62.5700217 , 0.21684658],   # Std values for training set normalization\n",
    "        \"val_mean\": [116.55829252, 159.9192156 , 157.07218772, 159.64730144, 232.93981733, \n",
    "                     140.17248727,  73.87612301, 104.39620349, 175.07042173, 0.59085459],    # Mean values for validation set normalization\n",
    "        \"val_std\": [52.10727386,  54.40206462,  79.55377193,  53.06491964, 48.64101637, \n",
    "                    109.12006317,  28.95246699,  56.29244445, 63.20585391, 0.21176251],     # Std values for validation set normalization\n",
    "        \"classwise_norm\": {\n",
    "            \"Other\": {\n",
    "                \"mean\": [110.14499, 138.28326, 117.67161, 138.71075, 233.10298, 95.24617, \n",
    "                         61.51174, 95.255585, 143.12497, 0.5121758],\n",
    "                \"std\": [49.80741, 58.08042, 73.33471, 56.73531, 46.3197, 107.53641, \n",
    "                        27.30666, 48.58509, 61.8592, 0.22473]\n",
    "            },\n",
    "            \"Maize\": {\n",
    "                \"mean\": [90.51625, 108.20322, 72.41352, 109.907, 203.1153, 24.78379, \n",
    "                         45.99272, 118.35542, 112.66669, 0.39319476],\n",
    "                \"std\": [53.35697, 45.1693, 41.97862, 46.84034, 85.41561, 37.93971, \n",
    "                        21.92598, 52.69628, 46.29754, 0.1780796]\n",
    "            },\n",
    "            \"Soybean\": {\n",
    "                \"mean\": [83.55291, 101.03232, 66.89771, 102.10976, 220.03397, 28.55189, \n",
    "                         47.94096, 116.89587, 103.48843, 0.36566228],\n",
    "                \"std\": [49.91921, 47.52045, 41.91468, 48.74042, 67.89334, 47.91879, \n",
    "                        20.7751, 53.59672, 48.15541, 0.18223667]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"filename\": \"csiss_street_view_crop_classification.csv\",  # Evaluation metrics to be used\n",
    "        \"class_mapping\": {\n",
    "            0: \"Background\",\n",
    "            1: \"Maize\",\n",
    "            2: \"Soybean\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['dataset']['train_csv'])\n",
    "val_df = pd.read_csv(config['dataset']['val_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsideCropImageDataset(\n",
    "    dataframe=train_df,\n",
    "    root_dir=config['dataset']['train_root_path'],\n",
    "    usage='train',\n",
    "    mean=config['dataset']['train_mean'], \n",
    "    std=config['dataset']['train_std'],  \n",
    "    # classwise_norm=config['dataset']['classwise_norm'], \n",
    "    use_ancillary=True\n",
    ")\n",
    "\n",
    "val_dataset = RoadsideCropImageDataset(\n",
    "    dataframe=val_df,\n",
    "    root_dir=config['dataset']['val_root_path'],\n",
    "    usage='val',\n",
    "    mean=config['dataset']['val_mean'], \n",
    "    std=config['dataset']['val_std'],  \n",
    "    # classwise_norm=config['dataset']['classwise_norm'], \n",
    "    use_ancillary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['validation']['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Uncertainty (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWithUncertainty(n_channels=config['model']['params']['in_channels'],\n",
    "                          n_classes=config['model']['params']['out_channels'],\n",
    "                          ancillary_data_dim=3)\n",
    "model_comp = ModelCompiler(model=model,\n",
    "                           params_init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.fit(\n",
    "    trainDataset=train_loader,\n",
    "    valDataset=val_loader,\n",
    "    epochs=config['training']['epochs'],\n",
    "    optimizer_name=config['training']['optimizer']['type'],\n",
    "    lr_init=config['training']['learning_rate'],\n",
    "    lr_policy='steplr',  # Use 'steplr' as specified in config\n",
    "    criterion=config['training']['criterion'],\n",
    "    log=True,  # Enable logging for TensorBoard\n",
    "    return_loss=False,\n",
    "    use_ancillary=True,\n",
    "    use_dropout=False,\n",
    "    **config['training']['scheduler']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWithUncertainty(n_channels=config['model']['params']['in_channels'],\n",
    "                          n_classes=config['model']['params']['out_channels'],\n",
    "                          ancillary_data_dim=3)\n",
    "model_comp = ModelCompiler(model=model,\n",
    "                           params_init=\"/workspace/notebook/outputs/UNetWithUncertainty_ep50/chkpt/50_checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.evaluate(dataloader=val_loader,\n",
    "                    num_classes=config['model']['params']['out_channels'],\n",
    "                    out_name=config['evaluation']['filename'],  \n",
    "                    class_mapping=config['evaluation']['class_mapping'],\n",
    "                    log_uncertainty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.predict_image(\"/workspace/data/data/all_sv_imgs/IMG_2022_004.jpg\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With only Crop Calendar (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWithFiLM(n_channels=config['model']['params']['in_channels'],\n",
    "                     n_classes=config['model']['params']['out_channels'],\n",
    "                     ancillary_data_dim=3,\n",
    "                     use_dropout=False)\n",
    "model_comp = ModelCompiler(model=model,\n",
    "                           params_init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.fit(\n",
    "    trainDataset=train_loader,\n",
    "    valDataset=val_loader,\n",
    "    epochs=config['training']['epochs'],\n",
    "    optimizer_name=config['training']['optimizer']['type'],\n",
    "    lr_init=config['training']['learning_rate'],\n",
    "    lr_policy='steplr',  # Use 'steplr' as specified in config\n",
    "    criterion=config['training']['criterion'],\n",
    "    # class_weights=config['training']['classwise_weights'],\n",
    "    log=True,  # Enable logging for TensorBoard\n",
    "    return_loss=False,\n",
    "    use_ancillary=True,\n",
    "    **config['training']['scheduler']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.evaluate(dataloader=val_loader,\n",
    "                    num_classes=config['model']['params']['out_channels'],\n",
    "                    class_mapping=config['evaluation']['class_mapping'],\n",
    "                    out_name=config['evaluation']['filename'],  \n",
    "                    log_uncertainty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Attention and Multiple FiLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWithAttention(n_channels=config['model']['params']['in_channels'],\n",
    "                     n_classes=config['model']['params']['out_channels'],\n",
    "                     ancillary_data_dim=3,\n",
    "                     dropout_rate=config['training']['dropout_rate'])\n",
    "model_comp = ModelCompiler(model=model,\n",
    "                           params_init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.fit(\n",
    "    trainDataset=train_loader,\n",
    "    valDataset=val_loader,\n",
    "    epochs=config['training']['epochs'],\n",
    "    optimizer_name=config['training']['optimizer']['type'],\n",
    "    lr_init=config['training']['learning_rate'],\n",
    "    lr_policy='steplr',  # Use 'steplr' as specified in config\n",
    "    criterion=config['training']['criterion'],\n",
    "    class_weights=config['training']['classwise_weights'],\n",
    "    log=True,  # Enable logging for TensorBoard\n",
    "    return_loss=False,\n",
    "    use_ancillary=True,\n",
    "    **config['training']['scheduler']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNetWithAttention(n_channels=config['model']['params']['in_channels'],\n",
    "#                           n_classes=config['model']['params']['out_channels'],\n",
    "#                           ancillary_data_dim=3)\n",
    "# model_comp = ModelCompiler(model=model,\n",
    "#                            params_init=\"/workspace/notebook/outputs-UWA-ep50-lr0.01-batch32/UNetWithAttention_ep50/chkpt/final_checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp.evaluate(dataloader=val_loader,\n",
    "                    num_classes=config['model']['params']['out_channels'],\n",
    "                    class_mapping=config['evaluation']['class_mapping'],\n",
    "                    out_name=config['evaluation']['filename'],  \n",
    "                    log_uncertainty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = model_comp.simple_predict(image_path=\"/workspace/data/all_sv_imgs/IMG_2022_279.jpg\",\n",
    "                                 csv_path=\"/workspace/data/masked_data_csiss/validation/validation_chipping_csv_w_anc.csv\",\n",
    "                                 save_path=\"/workspace/data/inference_result/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
