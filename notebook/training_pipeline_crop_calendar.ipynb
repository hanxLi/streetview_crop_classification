{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboard --quiet\n",
    "%pip install seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/cropClassification')\n",
    "sys.path.append('/workspace/cropClassification/model')\n",
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unet import UNetWithAncillary\n",
    "from compiler import ModelCompiler\n",
    "from dataloader import RoadsideCropImageDataset\n",
    "from loss import BalancedCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU with CUDA\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple M1/M2 GPU with MPS (Metal Performance Shaders)\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"type\": \"UNetWithAncillary\",  # Example model type, replace with your actual model class name\n",
    "        \"params\": {\n",
    "            \"in_channels\": 9,  # Since we are using 9-channel input images\n",
    "            \"out_channels\": 3   # Number of output classes for segmentation\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"params\": {\n",
    "                \"lr\": 0.01\n",
    "            }\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"StepLR\",\n",
    "            \"params\": {\n",
    "                \"step_size\": 10,\n",
    "                \"gamma\": 0.5\n",
    "            }\n",
    "        },\n",
    "        \"criterion\": BalancedCrossEntropyLoss(),  # or you can use your custom loss like BalancedCrossEntropyLoss\n",
    "        \"resume\" : False,\n",
    "        \"resume_epoch\" : None,\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 16\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_csv\": \"/workspace/data/data/masked_data_csiss/training/train_chipping_csv_w_anc.csv\",  # Path to the training DataFrame (includes npy file paths)\n",
    "        \"val_csv\": \"/workspace/data/data/masked_data_csiss/validation/validation_chipping_csv_w_anc.csv\", # Path to the validation DataFrame (includes npy file paths)\n",
    "        \"train_root_path\": \"/workspace/data/data/masked_data_csiss/training\",\n",
    "        \"val_root_path\": \"/workspace/data/data/masked_data_csiss/validation\",\n",
    "        \"image_column\": \"img_chip_path\",             # Column containing the image paths (npy files)\n",
    "        \"mask_column\": \"lbl_chip_path\",              # Column containing the mask paths\n",
    "        \"train_mean\": [93.8785585, 111.81092494, 76.94555781, 113.58929434, 206.93557473, 28.98472963,\n",
    "                       46.53684269, 113.74437327, 116.23856585],  # Mean values for training set normalization\n",
    "        \"train_std\": [53.24595916, 46.34658429, 45.91157286, 47.69937365, 82.32605363, 48.02506071,\n",
    "                       22.46416468, 52.31732116, 47.89290138],   # Std values for training set normalization\n",
    "        \"val_mean\": [88.62211239, 111.27628711, 75.19030815, 111.99622799, 223.21125121, 25.98495476, \n",
    "                     49.64829283, 115.91463906, 114.01573621],    # Mean values for validation set normalization\n",
    "        \"val_std\": [53.97481266, 46.47043658, 45.64034871, 48.20284577, 59.89659002, 41.7467965,\n",
    "                     20.82148233, 54.41768437, 47.68361442],     # Std values for validation set normalization\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"filename\": \"csiss_street_view_crop_classification.csv\",  # Evaluation metrics to be used\n",
    "        \"class_mapping\": {\n",
    "            0: \"Background\",\n",
    "            1: \"Maize\",\n",
    "            2: \"Soybean\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['dataset']['train_csv'])\n",
    "val_df = pd.read_csv(config['dataset']['val_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsideCropImageDataset(\n",
    "    dataframe=train_df,\n",
    "    root_dir=config['dataset']['train_root_path'],  # Root directory where images are stored\n",
    "    usage='train',  # Indicates training dataset\n",
    "    mean=config['dataset']['train_mean'],\n",
    "    std=config['dataset']['train_std'],\n",
    "    use_ancillary=True\n",
    ")\n",
    "\n",
    "val_dataset = RoadsideCropImageDataset(\n",
    "    dataframe=val_df,\n",
    "    root_dir=config['dataset']['val_root_path'],  # Root directory where images are stored\n",
    "    usage='val',  # Indicates validation dataset\n",
    "    mean=config['dataset']['val_mean'],\n",
    "    std=config['dataset']['val_std'],\n",
    "    use_ancillary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- GPU (CUDA) available ----------\n",
      "Total number of trainable parameters: 14.8M\n",
      "---------- UNetWithAncillary model compiled successfully ----------\n"
     ]
    }
   ],
   "source": [
    "model = UNetWithAncillary(n_channels=config['model']['params']['in_channels'],\n",
    "                          n_classes=config['model']['params']['out_channels'],\n",
    "                          ancillary_data_dim=3)\n",
    "model_comp = ModelCompiler(model=model,\n",
    "                           params_init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Start training --------------------------\n",
      "----------------------- [1/50] -----------------------\n",
      "Epoch Training Loss: 0.6175\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.5869\n",
      "Epoch 1 completed in 158.61 seconds\n",
      "----------------------- [2/50] -----------------------\n",
      "Epoch Training Loss: 0.5590\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.5823\n",
      "Epoch 2 completed in 159.94 seconds\n",
      "----------------------- [3/50] -----------------------\n",
      "Epoch Training Loss: 0.5365\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.5945\n",
      "Epoch 3 completed in 163.80 seconds\n",
      "----------------------- [4/50] -----------------------\n",
      "Epoch Training Loss: 0.5057\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.5077\n",
      "Epoch 4 completed in 134.97 seconds\n",
      "----------------------- [5/50] -----------------------\n",
      "Epoch Training Loss: 0.4844\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.6184\n",
      "Epoch 5 completed in 142.51 seconds\n",
      "----------------------- [6/50] -----------------------\n",
      "Epoch Training Loss: 0.5076\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.6545\n",
      "Epoch 6 completed in 146.30 seconds\n",
      "----------------------- [7/50] -----------------------\n",
      "Epoch Training Loss: 0.4836\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.6214\n",
      "Epoch 7 completed in 155.91 seconds\n",
      "----------------------- [8/50] -----------------------\n",
      "Epoch Training Loss: 0.4826\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.8460\n",
      "Epoch 8 completed in 164.93 seconds\n",
      "----------------------- [9/50] -----------------------\n",
      "Epoch Training Loss: 0.4691\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.6602\n",
      "Epoch 9 completed in 162.19 seconds\n",
      "----------------------- [10/50] -----------------------\n",
      "Epoch Training Loss: 0.4404\n",
      "Current Learning Rate: 0.010000\n",
      "Validation Loss: 0.7859\n",
      "Epoch 10 completed in 158.18 seconds\n",
      "----------------------- [11/50] -----------------------\n",
      "Epoch Training Loss: 0.4513\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.4655\n",
      "Epoch 11 completed in 157.95 seconds\n",
      "----------------------- [12/50] -----------------------\n",
      "Epoch Training Loss: 0.4386\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.4905\n",
      "Epoch 12 completed in 156.98 seconds\n",
      "----------------------- [13/50] -----------------------\n",
      "Epoch Training Loss: 0.4446\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.4616\n",
      "Epoch 13 completed in 156.76 seconds\n",
      "----------------------- [14/50] -----------------------\n",
      "Epoch Training Loss: 0.4191\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.5323\n",
      "Epoch 14 completed in 156.76 seconds\n",
      "----------------------- [15/50] -----------------------\n",
      "Epoch Training Loss: 0.4197\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.4369\n",
      "Epoch 15 completed in 159.98 seconds\n",
      "----------------------- [16/50] -----------------------\n",
      "Epoch Training Loss: 0.4276\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.5328\n",
      "Epoch 16 completed in 159.22 seconds\n",
      "----------------------- [17/50] -----------------------\n",
      "Epoch Training Loss: 0.4292\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.5288\n",
      "Epoch 17 completed in 163.57 seconds\n",
      "----------------------- [18/50] -----------------------\n",
      "Epoch Training Loss: 0.4113\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.5283\n",
      "Epoch 18 completed in 166.12 seconds\n",
      "----------------------- [19/50] -----------------------\n",
      "Epoch Training Loss: 0.4036\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.5614\n",
      "Epoch 19 completed in 162.99 seconds\n",
      "----------------------- [20/50] -----------------------\n",
      "Epoch Training Loss: 0.4296\n",
      "Current Learning Rate: 0.005000\n",
      "Validation Loss: 0.5664\n",
      "Epoch 20 completed in 163.86 seconds\n",
      "----------------------- [21/50] -----------------------\n",
      "Epoch Training Loss: 0.3932\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.4137\n",
      "Epoch 21 completed in 164.40 seconds\n",
      "----------------------- [22/50] -----------------------\n",
      "Epoch Training Loss: 0.3706\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.4049\n",
      "Epoch 22 completed in 167.64 seconds\n",
      "----------------------- [23/50] -----------------------\n",
      "Epoch Training Loss: 0.3735\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.4027\n",
      "Epoch 23 completed in 163.69 seconds\n",
      "----------------------- [24/50] -----------------------\n",
      "Epoch Training Loss: 0.3666\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.3914\n",
      "Epoch 24 completed in 163.57 seconds\n",
      "----------------------- [25/50] -----------------------\n",
      "Epoch Training Loss: 0.3605\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.3888\n",
      "Epoch 25 completed in 163.74 seconds\n",
      "----------------------- [26/50] -----------------------\n",
      "Epoch Training Loss: 0.3575\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.4214\n",
      "Epoch 26 completed in 163.52 seconds\n",
      "----------------------- [27/50] -----------------------\n",
      "Epoch Training Loss: 0.3731\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.4026\n",
      "Epoch 27 completed in 163.57 seconds\n",
      "----------------------- [28/50] -----------------------\n",
      "Epoch Training Loss: 0.3540\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.3646\n",
      "Epoch 28 completed in 163.49 seconds\n",
      "----------------------- [29/50] -----------------------\n",
      "Epoch Training Loss: 0.3584\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.4005\n",
      "Epoch 29 completed in 163.46 seconds\n",
      "----------------------- [30/50] -----------------------\n",
      "Epoch Training Loss: 0.3515\n",
      "Current Learning Rate: 0.002500\n",
      "Validation Loss: 0.3878\n",
      "Epoch 30 completed in 163.70 seconds\n",
      "----------------------- [31/50] -----------------------\n",
      "Epoch Training Loss: 0.3416\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3558\n",
      "Epoch 31 completed in 163.79 seconds\n",
      "----------------------- [32/50] -----------------------\n",
      "Epoch Training Loss: 0.3345\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3673\n",
      "Epoch 32 completed in 156.39 seconds\n",
      "----------------------- [33/50] -----------------------\n",
      "Epoch Training Loss: 0.3278\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3474\n",
      "Epoch 33 completed in 159.66 seconds\n",
      "----------------------- [34/50] -----------------------\n",
      "Epoch Training Loss: 0.3343\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3733\n",
      "Epoch 34 completed in 160.21 seconds\n",
      "----------------------- [35/50] -----------------------\n",
      "Epoch Training Loss: 0.3251\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3658\n",
      "Epoch 35 completed in 161.73 seconds\n",
      "----------------------- [36/50] -----------------------\n",
      "Epoch Training Loss: 0.3219\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3863\n",
      "Epoch 36 completed in 160.02 seconds\n",
      "----------------------- [37/50] -----------------------\n",
      "Epoch Training Loss: 0.3234\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3699\n",
      "Epoch 37 completed in 159.89 seconds\n",
      "----------------------- [38/50] -----------------------\n",
      "Epoch Training Loss: 0.3261\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3832\n",
      "Epoch 38 completed in 160.33 seconds\n",
      "----------------------- [39/50] -----------------------\n",
      "Epoch Training Loss: 0.3277\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3871\n",
      "Epoch 39 completed in 159.77 seconds\n",
      "----------------------- [40/50] -----------------------\n",
      "Epoch Training Loss: 0.3211\n",
      "Current Learning Rate: 0.001250\n",
      "Validation Loss: 0.3579\n",
      "Epoch 40 completed in 165.90 seconds\n",
      "----------------------- [41/50] -----------------------\n",
      "Epoch Training Loss: 0.3151\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3708\n",
      "Epoch 41 completed in 175.23 seconds\n",
      "----------------------- [42/50] -----------------------\n",
      "Epoch Training Loss: 0.3142\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3830\n",
      "Epoch 42 completed in 175.04 seconds\n",
      "----------------------- [43/50] -----------------------\n",
      "Epoch Training Loss: 0.3160\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3622\n",
      "Epoch 43 completed in 174.82 seconds\n",
      "----------------------- [44/50] -----------------------\n",
      "Epoch Training Loss: 0.3127\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3571\n",
      "Epoch 44 completed in 174.90 seconds\n",
      "----------------------- [45/50] -----------------------\n",
      "Epoch Training Loss: 0.3160\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3582\n",
      "Epoch 45 completed in 175.09 seconds\n",
      "----------------------- [46/50] -----------------------\n",
      "Epoch Training Loss: 0.3067\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3551\n",
      "Epoch 46 completed in 175.05 seconds\n",
      "----------------------- [47/50] -----------------------\n",
      "Epoch Training Loss: 0.3046\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3650\n",
      "Epoch 47 completed in 175.16 seconds\n",
      "----------------------- [48/50] -----------------------\n",
      "Epoch Training Loss: 0.3099\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3865\n",
      "Epoch 48 completed in 175.27 seconds\n",
      "----------------------- [49/50] -----------------------\n",
      "Epoch Training Loss: 0.3026\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3884\n",
      "Epoch 49 completed in 175.19 seconds\n",
      "----------------------- [50/50] -----------------------\n",
      "Epoch Training Loss: 0.3091\n",
      "Current Learning Rate: 0.000625\n",
      "Validation Loss: 0.3553\n",
      "Epoch 50 completed in 175.44 seconds\n",
      "-------------------------- Training finished in 8151s --------------------------\n"
     ]
    }
   ],
   "source": [
    "model_comp.fit(\n",
    "    trainDataset=train_loader,\n",
    "    valDataset=val_loader,\n",
    "    epochs=config['training']['epochs'],\n",
    "    optimizer_name=config['training']['optimizer']['type'],\n",
    "    lr_init=config['training']['learning_rate'],\n",
    "    lr_policy='steplr',  # Use 'steplr' as specified in config\n",
    "    criterion=config['training']['criterion'],\n",
    "    log=True,  # Enable logging for TensorBoard\n",
    "    return_loss=False,\n",
    "    use_ancillary=True,\n",
    "    **config['training']['scheduler']['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- GPU (CUDA) available ----------\n",
      "Total number of trainable parameters: 14.8M\n",
      "---------- Pre-trained UNetWithAncillary model compiled successfully ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/cropClassification/model/compiler.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params_init = torch.load(dir_params)\n"
     ]
    }
   ],
   "source": [
    "model = UNetWithAncillary(n_channels=config['model']['params']['in_channels'],\n",
    "                          n_classes=config['model']['params']['out_channels'],\n",
    "                          ancillary_data_dim=3)\n",
    "model_comp = ModelCompiler(model=model,\n",
    "                           params_init=\"/workspace/notebook/outputs/UNetWithAncillary_ep50/chkpt/50_checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Start evaluation ----------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_comp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevaluation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout_channels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevaluation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_mapping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/cropClassification/model/compiler.py:232\u001b[0m, in \u001b[0;36mModelCompiler.accuracy_evaluation\u001b[0;34m(self, eval_dataset, filename, num_classes, class_mapping)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------- Start evaluation ----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 232\u001b[0m \u001b[43mdo_accuracy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m duration_in_sec \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m.\u001b[39mseconds\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------- Evaluation finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration_in_sec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms ----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/cropClassification/model/eval.py:196\u001b[0m, in \u001b[0;36mdo_accuracy_evaluation\u001b[0;34m(model, dataloader, num_classes, class_mapping, out_name)\u001b[0m\n\u001b[1;32m    194\u001b[0m     images, labels, ancillary_data \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    195\u001b[0m     ancillary_data \u001b[38;5;241m=\u001b[39m ancillary_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mancillary_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    198\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/cropClassification/model/unet.py:224\u001b[0m, in \u001b[0;36mUNetWithAncillary.forward\u001b[0;34m(self, x, ancillary_data)\u001b[0m\n\u001b[1;32m    221\u001b[0m x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown4(x4)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Process ancillary data\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m ancillary_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mancillary_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mancillary_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, 1024 // factor]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m ancillary_processed \u001b[38;5;241m=\u001b[39m ancillary_processed\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Add spatial dimensions\u001b[39;00m\n\u001b[1;32m    226\u001b[0m ancillary_processed \u001b[38;5;241m=\u001b[39m ancillary_processed\u001b[38;5;241m.\u001b[39mexpand_as(x5)  \u001b[38;5;66;03m# Expand to match the size of x5\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "model_comp.accuracy_evaluation(eval_dataset=val_loader,\n",
    "                               filename=config['evaluation']['filename'], \n",
    "                               num_classes=config['model']['params']['out_channels'], \n",
    "                               class_mapping=config['evaluation']['class_mapping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
